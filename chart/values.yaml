# Values for configuring the deployment of TimescaleDB
# The charts README is at:
#    https://github.com/timescale/timescaledb-kubernetes/tree/master/charts/timescaledb-single
# Check out the various configuration options (administration guide) at:
#    https://github.com/timescale/timescaledb-kubernetes/blob/master/charts/timescaledb-single/admin-guide.md
cli: false
timescaledb-single:
  # disable the chart if an existing TimescaleDB instance is used
  enabled: true
  image:
    tag: pg12-ts2.1-latest
  # create only a ClusterIP service
  loadBalancer:
    enabled: false
  # number or TimescaleDB pods to spawn (default is 3, 1 for no HA)
  replicaCount: 1
  # backup is disabled by default, enable it
  # if you want to backup timescaleDB to s3
  # you can provide the s3 details on tobs install
  # in the user prompt or you can set s3 details in the
  # env variables for the following keys:
  # PGBACKREST_REPO1_S3_BUCKET
  # PGBACKREST_REPO1_S3_ENDPOINT
  # PGBACKREST_REPO1_S3_REGION
  # PGBACKREST_REPO1_S3_KEY
  # PGBACKREST_REPO1_S3_KEY_SECRET
  backup:
    enabled: false
  persistentVolumes:
    data:
      size: 150Gi
    wal:
      size: 20Gi

# Values for configuring the deployment of the Promscale Connector
# The charts README is at:
#   https://github.com/timescale/promscale/tree/master/helm-chart
promscale:
  enabled: true
  image: timescale/promscale:0.3.0
  # connection options
  connection:
    # the db name in which the metrics will be stored
    dbName: &metricDB postgres
    # user to connect to TimescaleDB with
    user: postgres
    password:
      # Name of a secret object (templated) containing the connection password
      # Key must be value of promscale.connection.user
      # Created by default if timescaledb-single.enabled=true
      secretTemplate: &dbPassSecret "{{ .Release.Name }}-credentials"
      timescaleDBSuperUserKey: PATRONI_SUPERUSER_PASSWORD
    host:
      # Host name (templated) of the database instance, default
      # to service created in timescaledb-single
      nameTemplate: &dbHost "{{ .Release.Name }}.{{ .Release.Namespace }}.svc.cluster.local"
    port: 5432

  # configuration options for the service exposed by promscale
  service:
    # we disable the load balancer by default, only a ClusterIP service
    # will get created
    loadBalancer:
      enabled: false
  resources:
    requests:
      # By default this should be enough for a cluster
      # with only a few pods
      memory: 2Gi
      cpu: 1
# Values for configuring the deployment of Prometheus
# The Prometheus chart from the prometheus-community is used
# the guide for it can be found at:
#   https://artifacthub.io/packages/helm/prometheus-community/prometheus
# with the default values documented at:
#   https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml
prometheus:
  enabled: true
  alertmanager:
    enabled: false
  pushgateway:
    enabled: false
  server:
    # during the upgrades rollingUpdate is creating chaos
    # as both old & new pod try to mount same PVC. So we use
    # Prometheus upgrade strategy as Recreate
    strategy:
      type: Recreate
    # we provide a config map which will have the
    # promscale connector already configured as
    # a remote endpoint for read/write (if enabled)
    # this overrides the config map the prometheus chart
    # creates
    configMapOverrideName: "prometheus-config"
    # values to be used in the override config map
    # for a Promscale remote_write and remote_read config
    timescaleRemote:
      # templated
      host: "{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc.cluster.local"
      protocol: http
      port: "9201"
      write:
        enabled: true
        # complete url = {protocol}://{host}:{port}/{endpoint}
        endpoint: "write"
        # write queue config
        # see: https://prometheus.io/docs/practices/remote_write/
        queue:
          max_shards: 30
      read:
        enabled: true
        # complete url = {protocol}://{host}:{port}/endpoint
        endpoint: "read"
        read_recent: true
  #You can add other configs from https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml here
  #For example, adding additional scrape jobs
  #extraScrapeConfigs: |
    #example of adding hypotetical scrape job for https://github.com/AICoE/prometheus-anomaly-detector
    #- job_name: prometheus-anomaly
    #  static_configs:
    #    - targets:
    #      - prometheus-anomaly-svc.default:8080

# Values for configuring the deployment of Grafana
# The Grafana Community chart is used and the guide for it
# can be found at:
#   https://github.com/grafana/helm-charts/blob/main/charts/grafana/README.md
grafana:
  enabled: true
  sidecar:
    datasources:
      enabled: true
    dashboards:
      enabled: true
      files:
        - dashboards/k8s-cluster.json
        - dashboards/k8s-hardware.json
  envFromSecret: "{{ .Release.Name }}-grafana-db"
  prometheus:
    datasource:
      enabled: true
      # By default url of data source is set to ts-prom connector instance
      # deployed with this chart. If a connector isn't used this should be
      # set to the prometheus-server.
      url: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc.cluster.local:9201"
  timescale:
    database:
      enabled: true
      host:  *dbHost
      port: 5432
      user: grafanadb
      pass: grafanadb
      dbName: *metricDB
      schema: grafanadb
      sslMode: require
    datasource:
      enabled: true
      user: grafana
      pass: grafana
      dbName: *metricDB
      sslMode: require
      # By default the url/host is set to the db instance deployed
      # with this chart
      host: *dbHost
      port: 5432
    adminUser: postgres
    adminPassSecret: *dbPassSecret

grafanaDBJob:
  resources: {}

#Enable PromLens  https://promlens.com/
#PromLens is a PromQL query builder, analyzer, and visualizer
promlens:
  enabled: true
  image: "promlabs/promlens:latest"
  # This default URL assumes access via port-forwarding to the connector. If using
  # a load balancer for the connector, change as appropriate
  # NOTE: the internal cluster address does not work here as requests are made by the browser.
  defaultPrometheusUrl:  "http://localhost:9201"
  loadBalancer:
    enabled: false

#Enable Promscale to connect to an existing database by providing the db-uri
timescaledbExternal:
  enabled: false
  db_uri: ""