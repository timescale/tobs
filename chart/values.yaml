# Values for configuring the deployment of TimescaleDB
# The charts README is at:
#    https://github.com/timescale/timescaledb-kubernetes/tree/master/charts/timescaledb-single
# Check out the various configuration options (administration guide) at:
#    https://github.com/timescale/timescaledb-kubernetes/blob/master/charts/timescaledb-single/admin-guide.md

# Indicates if tobs helm chart is installed using the tobs CLI
cli: false

# Override the deployment namespace
namespaceOverride: ""

# TimescaleDB single helm chart configuration
timescaledb-single:
  # disable the chart if an existing TimescaleDB instance is used
  enabled: true
  # TimescaleDB image tag
  image:
    tag: pg12-ts2.1-latest
  # create only a ClusterIP service
  loadBalancer:
    enabled: false
  # number or TimescaleDB pods to spawn (default is 3, 1 for no HA)
  replicaCount: 1
  # backup is disabled by default, enable it
  # if you want to backup timescaleDB to s3
  # you can provide the s3 details on tobs install
  # in the user prompt or you can set s3 details in the
  # env variables for the following keys:
  # PGBACKREST_REPO1_S3_BUCKET
  # PGBACKREST_REPO1_S3_ENDPOINT
  # PGBACKREST_REPO1_S3_REGION
  # PGBACKREST_REPO1_S3_KEY
  # PGBACKREST_REPO1_S3_KEY_SECRET
  backup:
    enabled: false
  # TimescaleDB PVC sizes
  persistentVolumes:
    data:
      size: 150Gi
    wal:
      size: 20Gi

# Values for configuring the deployment of the Promscale
# The charts README is at:
#   https://github.com/timescale/promscale/tree/master/helm-chart
promscale:
  enabled: true
  image: timescale/promscale:0.6.1
  # needs to be enabled for tracing support in Promscale
  # to expose traces port
  tracing:
    enabled: false
  # by default Promscale isn't configured to accept traces
  # to enable pass below arg
  # arg:
  # - -otlp-grpc-server-listen-address=:9202

  connection:
    # the db name in which the metrics will be stored
    dbName: &metricDB postgres
    # user to connect to TimescaleDB with
    user: postgres
    password:
      # Name of a secret object (templated) containing the connection password
      # Key must be value of promscale.connection.user
      # Created by default if timescaledb-single.enabled=true
      secretTemplate: &dbPassSecret "{{ .Release.Name }}-credentials"
      timescaleDBSuperUserKey: PATRONI_SUPERUSER_PASSWORD
    host:
      # Host name (templated) of the database instance, default
      # to service created in timescaledb-single
      nameTemplate: &dbHost "{{ .Release.Name }}.{{ .Release.Namespace }}.svc.cluster.local"
    port: 5432

  # configuration options for the service exposed by promscale
  service:
    # we disable the load balancer by default, only a ClusterIP service
    # will get created
    loadBalancer:
      enabled: false
  # Promscale deployment resource requests
  resources:
    requests:
      # By default this should be enough for a cluster
      # with only a few pods
      memory: 2Gi
      cpu: 1

# Enabling Kube-Prometheus will install
# Grafana & Prometheus into tobs as they
# are part of Kube-Prometheus already
kube-prometheus-stack:
  enabled: true
  fullnameOverride: "tobs-kube-prometheus"
  prometheus:
    prometheusSpec:
      scrapeInterval: "1m"
      scrapeTimeout: "10s"
      evaluationInterval: "1m"
      # Prometheus metric retention
      retention: 1d
      # The remote_read spec configuration for Prometheus.
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotereadspec
      remoteRead:
        # - {protocol}://{host}:{port}/{endpoint}
        - url: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc.cluster.local:9201/read"
          readRecent: true

      # The remote_write spec configuration for Prometheus.
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotewritespec
      remoteWrite:
        - url: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc.cluster.local:9201/write"

      # Prometheus pod storage spec
      storageSpec:
        # Using PersistentVolumeClaim
        # disable mount sub path, use the root directory of pvc
        disableMountSubPath: true
        volumeClaimTemplate:
          spec:
            accessModes:
              - "ReadWriteOnce"
            resources:
              requests:
                storage: 8Gi

      # We've enabled annotation-based scraping by default for backward-compatibility
      # and to support the largest number of use-cases out-of-the-box.
      # We encourage people to use ServiceMonitors and PodMonitors for new components.
      # See discussion in: https://github.com/prometheus-operator/prometheus-operator/issues/1547
      # and more info: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#prometheusioscrape
      additionalScrapeConfigs:
        - job_name: kubernetes-service-endpoints
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scrape
            - action: replace
              regex: (https?)
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              source_labels:
                - __address__
                - __meta_kubernetes_service_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_service_name
              target_label: kubernetes_name
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_node_name
              target_label: kubernetes_node
        - job_name: kubernetes-service-endpoints-slow
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
            - action: replace
              regex: (https?)
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              source_labels:
                - __address__
                - __meta_kubernetes_service_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_service_name
              target_label: kubernetes_name
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_node_name
              target_label: kubernetes_node
          scrape_interval: 5m
          scrape_timeout: 30s
        - job_name: kubernetes-services
          kubernetes_sd_configs:
            - role: service
          metrics_path: /probe
          params:
            module:
              - http_2xx
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_probe
            - source_labels:
                - __address__
              target_label: __param_target
            - replacement: blackbox
              target_label: __address__
            - source_labels:
                - __param_target
              target_label: instance
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - source_labels:
                - __meta_kubernetes_service_name
              target_label: kubernetes_name
        - job_name: kubernetes-pods
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scrape
            - action: replace
              regex: (https?)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              source_labels:
                - __address__
                - __meta_kubernetes_pod_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_name
              target_label: kubernetes_pod_name
            - action: drop
              regex: Pending|Succeeded|Failed
              source_labels:
                - __meta_kubernetes_pod_phase
        - job_name: kubernetes-pods-slow
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
            - action: replace
              regex: (https?)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              source_labels:
                - __address__
                - __meta_kubernetes_pod_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_name
              target_label: kubernetes_pod_name
            - action: drop
              regex: Pending|Succeeded|Failed
              source_labels:
                - __meta_kubernetes_pod_phase
          scrape_interval: 5m
          scrape_timeout: 30s

  # Values for configuring the deployment of Grafana
  # The Grafana Community chart is used and the guide for it
  # can be found at:
  #   https://github.com/grafana/helm-charts/blob/main/charts/grafana/README.md
  grafana:
    enabled: true
    sidecar:
      datasources:
        enabled: true
        label: tobs_datasource
        labelValue: "true"
        # Disable Prometheus datasource by default as
        # Promscale is the default datasource
        defaultDatasourceEnabled: false
      dashboards:
        # option to enable multi-cluster support
        # in Grafana dashboards by default disabled
        multicluster:
          global:
            enabled: false
        enabled: true
        files:
          - dashboards/k8s-cluster.json
          - dashboards/k8s-hardware.json
    adminPassword: ""
    envFromSecret: "{{ .Release.Name }}-grafana-db"
    prometheus:
      datasource:
        enabled: true
        # By default url of data source is set to ts-prom connector instance
        # deployed with this chart. If a connector isn't used this should be
        # set to the prometheus-server.
        url: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc.cluster.local:9201"
    timescale:
      database:
        enabled: true
        host: *dbHost
        port: 5432
        user: grafanadb
        pass: grafanadb
        dbName: *metricDB
        schema: grafanadb
        sslMode: require
      datasource:
        enabled: true
        user: grafana
        pass: grafana
        dbName: *metricDB
        sslMode: require
        # By default the url/host is set to the db instance deployed
        # with this chart
        host: *dbHost
        port: 5432
      adminUser: postgres
      adminPassSecret: *dbPassSecret

  # By default kube-state-metrics are scraped using
  # serviceMonitor disable annotation based scraping
  kube-state-metrics:
    prometheusScrape: false

  # By default node-exporter are scraped using
  # serviceMonitor disable annotation based scraping
  prometheus-node-exporter:
    service:
      annotations:
        prometheus.io/scrape: "false"

# GrafanaDB job config this job pre-configures Grafana with datasources and dashbaords
grafanaDBJob:
  resources: {}

# Enable PromLens  https://promlens.com/
# PromLens is a PromQL query builder, analyzer, and visualizer
promlens:
  enabled: true
  image: "promlabs/promlens:latest"
  # This default URL assumes access via port-forwarding to the connector. If using
  # a load balancer for the connector, change as appropriate
  # NOTE: the internal cluster address does not work here as requests are made by the browser.
  defaultPrometheusUrl:  "http://localhost:9201"
  loadBalancer:
    enabled: false

# Enable OpenTelemetry Operator
# If using tobs CLI you can enable otel with --enable-opentelemetry flag
opentelemetryOperator:
  enabled: false
  jaegerPromscaleQuery:
    image: timescale/jaeger-query-proxy:0.0.1
    endpoint: "{{ .Release.Name }}-jaeger-promscale.{{ .Release.Namespace }}.svc.cluster.local:16686"
    args:
      - --grpc-storage-plugin.configuration-file=/configs/jaeger-promscale-query.yaml
    config: |
      tls: false
      grpc-server: "{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc.cluster.local:9202"
      connection-timeout: 1h

    service:
      loadBalancer:
        enabled: false

# Enable Promscale to connect to an existing database by providing the db-uri
timescaledbExternal:
  enabled: false
  db_uri: ""