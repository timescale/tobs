# Values for configuring the deployment of TimescaleDB
# The charts README is at:
#    https://github.com/timescale/timescaledb-kubernetes/tree/master/charts/timescaledb-single
# Check out the various configuration options (administration guide) at:
#    https://github.com/timescale/timescaledb-kubernetes/blob/master/charts/timescaledb-single/admin-guide.md

# Indicates if tobs helm chart is installed using the tobs CLI
cli: false

# Override the deployment namespace
namespaceOverride: ""

# TimescaleDB single helm chart configuration
timescaledb-single:
  # disable the chart if an existing TimescaleDB instance is used
  enabled: &dbEnabled true
  
  # override default helm chart image to use one with newer promscale_extension
  image:
    repository: timescale/timescaledb-ha
    tag: pg14.3-ts2.7.0-p0
    pullPolicy: IfNotPresent

  # create only a ClusterIP service
  loadBalancer:
    enabled: false
  # number or TimescaleDB pods to spawn (default is 3, 1 for no HA)
  replicaCount: 1
  # backup is disabled by default, enable it
  # if you want to backup timescaleDB to s3
  # you can provide the s3 details on tobs install
  # in the user prompt or you can set s3 details in the
  # env variables for the following keys:
  # PGBACKREST_REPO1_S3_BUCKET
  # PGBACKREST_REPO1_S3_ENDPOINT
  # PGBACKREST_REPO1_S3_REGION
  # PGBACKREST_REPO1_S3_KEY
  # PGBACKREST_REPO1_S3_KEY_SECRET
  backup:
    enabled: false
  # TimescaleDB PVC sizes
  persistentVolumes:
    data:
      size: 150Gi
    wal:
      size: 20Gi
  ## TimescaleDB resource requests
  resources:
    requests:
      cpu: 100m
      memory: 2Gi


# Values for configuring the deployment of the Promscale
# The charts README is at:
#   https://github.com/timescale/promscale/tree/master/helm-chart
promscale:
  enabled: true
  image: timescale/promscale:0.12.1
  # needs to be enabled for tracing support in Promscale
  # to expose traces port, add tracing args to Promscale
  openTelemetry:
    enabled: &otelEnabled true
  # to pass extra args
  extraArgs:
    - "--metrics.high-availability=true"

  extraEnv:
    - name: "TOBS_TELEMETRY_INSTALLED_BY"
      value: "helm"
    - name: "TOBS_TELEMETRY_VERSION"
      value: "{{ .Chart.Version }}"
    - name: "TOBS_TELEMETRY_TRACING_ENABLED"
      value: *otelEnabled
    - name: "TOBS_TELEMETRY_TIMESCALEDB_ENABLED"
      value: *dbEnabled

  serviceMonitor:
    enabled: true

  prometheus:
    # turn off annotation-based scraping of promscale itself, user the serviceMonitor instead.
    annotations:
      # TODO(paulfantom): this can be removed when https://github.com/timescale/promscale/issues/1344 is fixed
      prometheus.io/scrape: "false"

  ## Note:

  # If you are providing your own secret name, do
  # not forget to configure at below connectionSecretName

  # selector used to provision your own Secret containing connection details
  # Use this option with caution

  # if you are adding a conn string here do not forget
  # to add the same for kube-prometheus.grafana.timescale.adminPassSecret
  connectionSecretName: ""

  ## Note:

  # If you using tobs deploy TimescaleDB do not configure below
  # any connection details below as tobs will take care of it.

  # connection details to connect to a target db
  connection:
    # Database connection settings. If `uri` is not
    # set then the specific user, pass, host, port and
    # sslMode properties are used.
    uri: ""
    # the db name in which the metrics will be stored
    dbName: &metricDB postgres
    # user to connect to TimescaleDB with
    user: postgres
    # empty password string will be populated automatically with a database password
    password: ""
    # Host name (templated) of the database instance, default
    # to service created in timescaledb-single
    host: &dbHost "{{ .Release.Name }}.{{ .Release.Namespace }}.svc"
    port: 5432
    sslMode: require

  # Promscale deployment resource requests
  resources:
    requests:
      # By default this should be enough for a cluster
      # with only a few pods
      memory: 500Mi
      cpu: 30m

# Enabling Kube-Prometheus will install
# Grafana & Prometheus into tobs as they
# are part of Kube-Prometheus already
kube-prometheus-stack:
  enabled: true
  fullnameOverride: "tobs-kube-prometheus"
  alertmanager:
    alertmanagerSpec:
      replicas: 3
      ## AlertManager resource requests
      resources:
        limits:
          memory: 100Mi
          cpu: 100m
        requests:
          memory: 50Mi
          cpu: 4m
  prometheusOperator:
    ## Prometheus sidecar config reloader container resource limits
    configReloaderCpu: 100m
    configReloaderMemory: 50Mi
    ## Prometheus Operator resource requests
    resources:
      limits:
        memory: 200Mi
        cpu: 100m
      requests:
        memory: 100Mi
        cpu: 10m
  prometheus:
    prometheusSpec:
      scrapeInterval: "1m"
      scrapeTimeout: "10s"
      evaluationInterval: "1m"
      # Prometheus metric retention
      retention: 1d
      # Number of replicas of each shard to deploy for a Prometheus deployment.
      replicas: 2
      ## Prometheus container retention
      resources:
        requests:
          memory: 400Mi
          cpu: 40m

      replicaExternalLabelName: "__replica__"
      # Promscale requires a cluster label to be present for high availability mode.
      prometheusExternalLabelName: "cluster"
      # The remote_read spec configuration for Prometheus.
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotereadspec
      remoteRead:
        # - {protocol}://{host}:{port}/{endpoint}
        - url: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9201/read"
          readRecent: true

      # The remote_write spec configuration for Prometheus.
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotewritespec
      remoteWrite:
        - url: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9201/write"

      # Prometheus pod storage spec
      storageSpec:
        # Using PersistentVolumeClaim
        # disable mount sub path, use the root directory of pvc
        disableMountSubPath: true
        volumeClaimTemplate:
          spec:
            accessModes:
              - "ReadWriteOnce"
            resources:
              requests:
                storage: 8Gi

      # We've enabled annotation-based scraping by default for backward-compatibility
      # and to support the largest number of use-cases out-of-the-box.
      # We encourage people to use ServiceMonitors and PodMonitors for new components.
      # See discussion in: https://github.com/prometheus-operator/prometheus-operator/issues/1547
      # and more info: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#prometheusioscrape
      additionalScrapeConfigs:
        - job_name: kubernetes-service-endpoints
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scrape
            - action: replace
              regex: (https?)
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              source_labels:
                - __address__
                - __meta_kubernetes_service_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_service_name
              target_label: kubernetes_name
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_node_name
              target_label: kubernetes_node
        - job_name: kubernetes-service-endpoints-slow
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
            - action: replace
              regex: (https?)
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              source_labels:
                - __address__
                - __meta_kubernetes_service_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_service_name
              target_label: kubernetes_name
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_node_name
              target_label: kubernetes_node
          scrape_interval: 5m
          scrape_timeout: 30s
        - job_name: kubernetes-services
          kubernetes_sd_configs:
            - role: service
          metrics_path: /probe
          params:
            module:
              - http_2xx
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_probe
            - source_labels:
                - __address__
              target_label: __param_target
            - replacement: blackbox
              target_label: __address__
            - source_labels:
                - __param_target
              target_label: instance
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - source_labels:
                - __meta_kubernetes_service_name
              target_label: kubernetes_name
        - job_name: kubernetes-pods
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scrape
            - action: replace
              regex: (https?)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              source_labels:
                - __address__
                - __meta_kubernetes_pod_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_name
              target_label: kubernetes_pod_name
            - action: drop
              regex: Pending|Succeeded|Failed
              source_labels:
                - __meta_kubernetes_pod_phase
        - job_name: kubernetes-pods-slow
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
            - action: replace
              regex: (https?)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              source_labels:
                - __address__
                - __meta_kubernetes_pod_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_name
              target_label: kubernetes_pod_name
            - action: drop
              regex: Pending|Succeeded|Failed
              source_labels:
                - __meta_kubernetes_pod_phase
          scrape_interval: 5m
          scrape_timeout: 30s

  # Values for configuring the deployment of Grafana
  # The Grafana Community chart is used and the guide for it
  # can be found at:
  #   https://github.com/grafana/helm-charts/blob/main/charts/grafana/README.md
  grafana:
    enabled: true
    # TODO(paulfantom): remove with kube-prometheus bump
    image:
      repository: grafana/grafana
      tag: 9.0.2
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 200m
        memory: 400Mi
      requests:
        cpu: 50m
        memory: 250Mi
    sidecar:
      datasources:
        enabled: true
        label: tobs_datasource
        labelValue: "true"
        # Disable Prometheus datasource by default as
        # Promscale is the default datasource
        defaultDatasourceEnabled: false
      dashboards:
        # option to enable multi-cluster support
        # in Grafana dashboards by default disabled
        multicluster:
          global:
            enabled: false
        enabled: true
        files:
          - dashboards/k8s-cluster.json
          - dashboards/k8s-hardware.json
          - dashboards/apm-dependencies.json
          - dashboards/apm-home.json
          - dashboards/apm-service-dependencies-downstream.json
          - dashboards/apm-service-dependencies-upstream.json
          - dashboards/apm-service-overview.json
          - dashboards/promscale.json
    adminUser: admin
    # To configure password externally refer to https://github.com/grafana/helm-charts/blob/6578497320d3c4672bab3a3c7fd38dffba1c9aba/charts/grafana/values.yaml#L340-L345
    adminPassword: ""
    persistence:
      type: pvc
      enabled: true
      accessModes:
        - ReadWriteOnce
    prometheus:
      datasource:
        enabled: true
        # By default url of data source is set to ts-prom connector instance
        # deployed with this chart. If a connector isn't used this should be
        # set to the prometheus-server.
        url: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9201"
    timescale:
      datasource:
        enabled: true
        user: grafana
        # leaving password empty will cause helm to generate a random password
        pass: ""
        dbName: *metricDB
        sslMode: require
        # By default the url/host is set to the db instance deployed
        # with this chart
        host: *dbHost
        port: 5432
    jaeger:
      # Endpoint for integrating jaeger datasource in grafana. This should point to HTTP endpoint, not gRPC.
      promscaleTracesQueryEndPoint: "{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9201"

  kube-state-metrics:
    # By default kube-state-metrics are scraped using
    # serviceMonitor disable annotation based scraping
    prometheusScrape: false
    resources:
      limits:
        cpu: 100m
        memory: 200Mi
      requests:
        cpu: 10m
        memory: 30Mi

  prometheus-node-exporter:
    # By default node-exporter are scraped using
    # serviceMonitor disable annotation based scraping
    service:
      annotations:
        prometheus.io/scrape: "false"
    resources:
      limits:
        cpu: 250m
        memory: 180Mi
      requests:
        cpu: 30m
        memory: 50Mi

# Enable PromLens  https://promlens.com/
# PromLens is a PromQL query builder, analyzer, and visualizer
promlens:
  enabled: false
  image: "promlabs/promlens:latest"
  # This default URL assumes access via port-forwarding to the connector. If using
  # a load balancer for the connector, change as appropriate
  # NOTE: the internal cluster address does not work here as requests are made by the browser.
  defaultPrometheusUrl:  "http://localhost:9201"
  loadBalancer:
    enabled: false

# Enable OpenTelemetry Operator
# If using tobs CLI you can enable otel with --enable-opentelemetry flag
opentelemetryOperator:
  enabled: *otelEnabled
  resources:
    limits:
      cpu: 50m
      memory: 260Mi
    requests:
      cpu: 5m
      memory: 130Mi
  collector:
    # The default otel collector that will be deployed by helm once
    # the otel operator is in running state
    config: |
      receivers:
        jaeger:
          protocols:
            grpc:
            thrift_http:

        otlp:
          protocols:
            grpc:
            http:

      exporters:
        logging:
        otlp:
          endpoint: "{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9202"
          compression: none
          tls:
            insecure: true
        prometheusremotewrite:
          endpoint: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9201/write"
          tls:
            insecure: true

      processors:
        batch:

      service:
        pipelines:
          traces:
            receivers: [jaeger, otlp]
            exporters: [logging, otlp]
            processors: [batch]
          metrics:
            receivers: [otlp]
            processors: [batch]
            exporters: [prometheusremotewrite]
