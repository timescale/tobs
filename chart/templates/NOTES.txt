###############################################################################
üëãüèΩ Welcome to tobs, The Observability Stack for Kubernetes

‚ú® Auto-configured and deployed:
{{- if index  .Values "kube-prometheus-stack" "enabled" }}
üî• Kube-Prometheus
{{- end }}
{{- if index .Values "timescaledb-single" "enabled" }}
üêØ TimescaleDB
{{- else }}
üêØ Connecting to an External TimescaleDB
{{- end }}
{{- if index .Values "promscale" "enabled" }}
ü§ù Promscale
{{- end }}
{{- if .Values.promlens.enabled }}
üßê PromLens
{{- end }}
{{- if index .Values "kube-prometheus-stack" "grafana" "enabled" }}
üìà Grafana
{{- end }}
{{- if index .Values "opentelemetryOperator" "enabled" }}
üöÄ OpenTelemetry
üéØ Jaeger
{{- end }}

{{- $prometheus := index .Values "kube-prometheus-stack" "prometheus" }}
{{- $kubePrometheus := index .Values "kube-prometheus-stack" }}
{{ if $prometheus.enabled }}
###############################################################################
üî• PROMETHEUS NOTES:
###############################################################################
{{ if $prometheus.enabled }}
Prometheus can be accessed via port {{ $prometheus.service.port }} on the following DNS name from within your cluster:
{{ $kubePrometheus.fullnameOverride }}-prometheus.{{ .Release.Namespace }}.svc.cluster.local
{{ if $prometheus.ingress.enabled -}}
Server URL(s) from outside the cluster:
{{- range $prometheus.ingress.hosts }}
http://{{ . }}
{{- end }}
{{- else }}
Get the Prometheus server URL by running these commands in the same shell:
{{- if contains "NodePort" $prometheus.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ $kubePrometheus.fullnameOverride }}-prometheus)
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" $prometheus.service.type }}
  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
        You can watch the status of by running 'kubectl get svc --namespace {{ .Release.Namespace }} -w {{ $kubePrometheus.fullnameOverride }}-prometheus'

  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ $kubePrometheus.fullnameOverride }}-prometheus -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
  echo http://$SERVICE_IP:{{ $prometheus.service.port }}
{{- else if contains "ClusterIP"  $prometheus.service.type }}
{{- if .Values.cli }}
  tobs {{- template "tobs.cliOptions" . }} prometheus port-forward
{{- else }}
  export SERVICE_NAME=$(kubectl get services --namespace {{ .Release.Namespace }} -l "app={{ $kubePrometheus.fullnameOverride }}-prometheus,release={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace {{ .Release.Namespace }} port-forward service/$SERVICE_NAME 9090:{{ $prometheus.service.port }}
{{- end }}
{{- end }}
{{- end }}

{{- if $prometheus.prometheusSpec.storageSpec }}
{{- else }}
WARNING! Persistence is disabled on Prometheus server
         You will lose your data when the Server pod is terminated.
         (Data will still persist in TimescaleDB.)
{{- end }}
{{- end }}

{{ if $kubePrometheus.alertmanager.enabled }}
The Prometheus alertmanager can be accessed via port {{ $kubePrometheus.alertmanager.service.port }} on the following DNS name from within your cluster:
{{ $kubePrometheus.fullnameOverride }}-alertmanager.{{ .Release.Namespace }}.svc.cluster.local

{{ if $kubePrometheus.alertmanager.ingress.enabled -}}
From outside the cluster, the alertmanager URL(s) are:
{{- range $kubePrometheus.alertmanager.ingress.hosts }}
http://{{ . }}
{{- end }}
{{- else }}
Get the Alertmanager URL by running these commands in the same shell:
{{- if contains "NodePort" $kubePrometheus.alertmanager.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ $kubePrometheus.fullnameOverride }}-alertmanager
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" $kubePrometheus.alertmanager.service.type }}
  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
        You can watch the status of by running 'kubectl get svc --namespace {{ .Release.Namespace }} -w {{ $kubePrometheus.fullnameOverride }}-alertmanager'

  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ $kubePrometheus.fullnameOverride }}-alertmanager -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
  echo http://$SERVICE_IP:{{ $kubePrometheus.alertmanager.service.port }}
{{- else if contains "ClusterIP" $kubePrometheus.alertmanager.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app=alertmanager,alertmanager={{ $kubePrometheus.fullnameOverride }}-alertmanager" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 9093
{{- end }}
{{- end }}

{{- if $kubePrometheus.alertmanager.storage }}
{{- else }}
WARNING! Persistence is disabled on AlertManager.
         You will lose your data when the AlertManager pod is terminated.
{{- end }}
{{- end }}

{{- end }}
{{- if or .Values "timescaledb-single" "enabled" .Values.timescaledbExternal.enabled }}
{{- $tsEnv := (set (set (deepCopy .) "Values" (index .Values "timescaledb-single")) "Chart" (dict "Name" "timescaledb")) }}

###############################################################################
üêØ TIMESCALEDB NOTES:
###############################################################################

{{- if index .Values "timescaledb-single" "enabled" }}

TimescaleDB can be accessed via port 5432 on the following DNS name from within your cluster:
{{ template "clusterName" $tsEnv }}.{{ .Release.Namespace }}.svc.cluster.local
{{- else }}

Connecting to an external TimescaleDB, As external DB URI has been configured during the installation.
{{- end }}

To get your password for superuser run:

{{- if .Values.cli }}
    tobs {{- template "tobs.cliOptions" . }} timescaledb get-password -U <user>
{{- else }}
    # superuser password
    PGPASSWORD_POSTGRES=$(kubectl get secret --namespace {{ .Release.Namespace }} {{ template "timescaledb.fullname" $tsEnv }}-passwords -o jsonpath="{.data.postgres}" | base64 --decode)

    # admin password
    PGPASSWORD_ADMIN=$(kubectl get secret --namespace {{ .Release.Namespace }} {{ template "timescaledb.fullname" $tsEnv }}-passwords -o jsonpath="{.data.admin}" | base64 --decode)
{{- end }}

To connect to your database, chose one of these options:

1. Run a postgres pod and connect using the psql cli:

{{- if .Values.cli }}
    tobs {{- template "tobs.cliOptions" . }} timescaledb connect -U <user>
{{- else }}
    # login as superuser
    kubectl run -i --tty --rm psql --image=postgres \
      --env "PGPASSWORD=$PGPASSWORD_POSTGRES" \
      --command -- psql -U postgres \
      -h {{ template "clusterName" $tsEnv }}.{{ .Release.Namespace }}.svc.cluster.local postgres

    # login as admin
    kubectl run -i --tty --rm psql --image=postgres \
      --env "PGPASSWORD=$PGPASSWORD_ADMIN" \
      --command -- psql -U admin \
      -h {{ template "clusterName" $tsEnv }}.{{ .Release.Namespace }}.svc.cluster.local postgres
{{- end }}

2. Directly execute a psql session on the master node

{{- if .Values.cli }}
   tobs {{- template "tobs.cliOptions" . }} timescaledb connect -m
{{- else }}
   MASTERPOD="$(kubectl get pod -o name --namespace {{ .Release.Namespace }} -l release={{ .Release.Name }},role=master)"
   kubectl exec -i --tty --namespace {{ .Release.Namespace }} ${MASTERPOD} -- psql -U postgres
{{- end }}
{{- end }}

{{ if .Values.promlens.enabled }}
###############################################################################
üßê PROMLENS NOTES:
###############################################################################
{{- $tsPromEnv := (set (set (deepCopy .) "Values" (index .Values "promscale")) "Chart" (dict "Name" "promscale"))  }}
   PromLens is a PromQL query builder, analyzer, and visualizer.

   You can access PromLens via a local browser by executing:
{{- if .Values.cli }}
    tobs {{- template "tobs.cliOptions" . }} promlens port-forward
{{- else }}
      kubectl --namespace {{ .Release.Namespace }} port-forward service/{{ .Release.Name }}-promlens 8081:80
      kubectl --namespace {{ .Release.Namespace }} port-forward service/{{ include "promscale.fullname" $tsPromEnv |trunc 53 }}-connector 9201:{{ index .Values "promscale" "service" "port"}}
      (Note: You have to port-forward both PromLens and Promscale at the same time)
{{- end }}

   Then you can point your browser to http://127.0.0.1:8081/.

{{- end -}}

{{- if .Values.opentelemetryOperator.enabled }}
###############################################################################
üöÄ  OPENTELEMETRY NOTES:
###############################################################################

    The OpenTelemetry collector is deployed to collect traces.

    OpenTelemetry collector can be accessed with the following DNS name from within your cluster:
    {{ .Release.Name }}-opentelemetry-collector.{{ .Release.Namespace }}.svc.cluster.local

{{- end -}}

{{- if .Values.opentelemetryOperator.enabled }}
###############################################################################
üéØ JAEGER NOTES:
###############################################################################

    Jaeger visualizes traces.

    You can access Jaeger via a local browser by executing:
{{- if .Values.cli }}
    tobs {{- template "tobs.cliOptions" . }} jaeger port-forward
{{- else }}
      kubectl --namespace {{ .Release.Namespace }} port-forward service/{{ .Release.Name }}-jaeger-promscale 16686:16686
{{- end }}

    Then you can point your browser to http://127.0.0.1:16686/.

{{- end -}}

{{ if and $kubePrometheus.enabled $kubePrometheus.grafana.enabled }}
###############################################################################
üìà GRAFANA NOTES:
###############################################################################
{{- $grafana := $kubePrometheus.grafana }}

1. The Grafana server can be accessed via port {{ $grafana.service.port }} on
   the following DNS name from within your cluster:
   {{ .Release.Name }}-grafana.{{ .Release.Namespace  }}.svc.cluster.local

   You can access grafana locally by executing:
{{- if .Values.cli }}
    tobs {{- template "tobs.cliOptions" . }} grafana port-forward
{{- else }}
      kubectl --namespace {{ .Release.Namespace }} port-forward service/{{ .Release.Name}}-grafana 8080:{{ $grafana.service.port }}
{{- end }}

   Then you can point your browser to http://127.0.0.1:8080/.

{{- if not (or $grafana.persistence.enabled $grafana.timescale.database.enabled) }}

2. Get your '{{ $grafana.adminUser }}' user password by running:
{{- if .Values.cli }}
    tobs {{- template "tobs.cliOptions" . }} grafana get-password
{{- else }}
    kubectl get secret --namespace {{ .Release.Namespace }} {{ .Release.Name }}-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
{{- end }}

WARNING! Persistence is disabled!!! You will lose your data when
         the Grafana pod is terminated.
{{- end }}
{{- if or $grafana.persistence.enabled $grafana.timescale.database.enabled }}
{{- if .Release.IsInstall }}

2. The '{{ $grafana.adminUser }}' user password can be retrieved by:
{{- if .Values.cli }}
    tobs {{- template "tobs.cliOptions" . }} grafana get-password
{{- else }}
    kubectl get secret --namespace {{ .Release.Namespace }} {{ .Release.Name }}-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
{{- end }}
{{- end -}}
{{- if .Release.IsUpgrade }}

2. Persistence is enabled, and you did an upgrade. If you don't have the password
   for '{{ $grafana.adminUser }}', it can not be retrieved again, you need
   to reset it (see 3.)
{{- end -}}
{{- end }}

3. You can reset the admin user password with grafana-cli from inside the pod.
{{- if .Values.cli }}
    tobs {{- template "tobs.cliOptions" . }} grafana change-password <password-you-want-to-set>
{{- else }}
   First attach yourself to the grafana container:
    GRAFANAPOD="$(kubectl get pod -o name --namespace {{ .Release.Namespace }} -l app.kubernetes.io/name=grafana)"
    kubectl exec -it ${GRAFANAPOD} -c grafana -- /bin/sh

   And then execute in the shell:
    grafana-cli admin reset-admin-password <password-you-want-to-set>
{{- end }}
{{- end }}

üöÄ Happy observing!


