# Values for configuring the deployment of TimescaleDB
# The charts README is at:
#    https://github.com/timescale/timescaledb-kubernetes/tree/master/charts/timescaledb-single
# Check out the various configuration options (administration guide) at:
#    https://github.com/timescale/timescaledb-kubernetes/blob/master/charts/timescaledb-single/admin-guide.md

# Indicates if tobs helm chart is installed using the tobs CLI
cli: false

# Override the deployment namespace
namespaceOverride: ""

# TimescaleDB single helm chart configuration
timescaledb-single:
  # disable the chart if an existing TimescaleDB instance is used
  enabled: &dbEnabled true

  # override default helm chart image to use one with newer promscale_extension
  image:
    repository: timescale/timescaledb-ha
    tag: pg14.4-ts2.7.2-p0
    pullPolicy: IfNotPresent

  # create only a ClusterIP service
  loadBalancer:
    enabled: false
  # number or TimescaleDB pods to spawn (default is 3, 1 for no HA)
  replicaCount: 1
  # backup is disabled by default, enable it
  # if you want to backup timescaleDB to s3
  # you can provide the s3 details on tobs install
  # in the user prompt or you can set s3 details in the
  # env variables for the following keys:
  # PGBACKREST_REPO1_S3_BUCKET
  # PGBACKREST_REPO1_S3_ENDPOINT
  # PGBACKREST_REPO1_S3_REGION
  # PGBACKREST_REPO1_S3_KEY
  # PGBACKREST_REPO1_S3_KEY_SECRET
  backup:
    enabled: false
  # TimescaleDB PVC sizes
  persistentVolumes:
    data:
      size: 150Gi
    wal:
      size: 20Gi
  ## TimescaleDB resource requests
  resources: null

# Values for configuring the deployment of the Promscale
# The charts README is at:
#   https://github.com/timescale/promscale/tree/master/helm-chart
promscale:
  enabled: true
  image: timescale/promscale:0.13.0
  # to pass extra args
  extraArgs:
    - "--metrics.high-availability=true"

  extraEnv:
    - name: "TOBS_TELEMETRY_INSTALLED_BY"
      value: "helm"
    - name: "TOBS_TELEMETRY_VERSION"
      value: "{{ .Chart.Version }}"
    - name: "TOBS_TELEMETRY_TRACING_ENABLED"
      value: "true"
    - name: "TOBS_TELEMETRY_TIMESCALEDB_ENABLED"
      value: *dbEnabled

  serviceMonitor:
    enabled: true

  ## Note:

  # If you are providing your own secret name, do
  # not forget to configure at below connectionSecretName

  # selector used to provision your own Secret containing connection details
  # Use this option with caution

  # if you are adding a conn string here do not forget
  # to add the same for kube-prometheus.grafana.timescale.adminPassSecret
  connectionSecretName: ""

  ## Note:

  # If you using tobs deploy TimescaleDB do not configure below
  # any connection details below as tobs will take care of it.

  # connection details to connect to a target db
  connection:
    # Database connection settings. If `uri` is not
    # set then the specific user, pass, host, port and
    # sslMode properties are used.
    uri: ""
    # the db name in which the metrics will be stored
    dbName: &metricDB postgres
    # user to connect to TimescaleDB with
    user: postgres
    # empty password string will be populated automatically with a database password
    password: ""
    # Host name (templated) of the database instance, default
    # to service created in timescaledb-single
    host: &dbHost "{{ .Release.Name }}.{{ .Release.Namespace }}.svc"
    port: 5432
    sslMode: require

  # Promscale deployment resource requests
  resources: null

# Enabling Kube-Prometheus will install
# Grafana & Prometheus into tobs as they
# are part of Kube-Prometheus already
kube-prometheus-stack:
  enabled: true
  fullnameOverride: "tobs-kube-prometheus"
  alertmanager:
    alertmanagerSpec:
      image:
        repository: quay.io/prometheus/alertmanager
        tag: v0.24.0
      replicas: 3
      ## AlertManager resource requests
      resources: null
  prometheusOperator:
    image:
      repository: quay.io/prometheus-operator/prometheus-operator
      tag: v0.58.0
      pullPolicy: IfNotPresent
    ## Prometheus config reloader configuration
    prometheusConfigReloader:
      # image to use for config and rule reloading
      image:
        repository: quay.io/prometheus-operator/prometheus-config-reloader
        tag: v0.58.0
      # resource config for prometheusConfigReloader
      resources: null
    ## Prometheus Operator resource requests
    resources: null
  prometheus:
    prometheusSpec:
      image:
        repository: quay.io/prometheus/prometheus
        tag: v2.37.0
      scrapeInterval: "1m"
      scrapeTimeout: "10s"
      evaluationInterval: "1m"
      # Prometheus metric retention
      retention: 1d
      # Number of replicas of each shard to deploy for a Prometheus deployment.
      replicas: 2
      ## Prometheus container retention
      resources: null

      replicaExternalLabelName: "__replica__"
      # Promscale requires a cluster label to be present for high availability mode.
      prometheusExternalLabelName: "cluster"
      # The remote_read spec configuration for Prometheus.
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotereadspec
      remoteRead:
        # - {protocol}://{host}:{port}/{endpoint}
        - url: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9201/read"
          readRecent: true

      # The remote_write spec configuration for Prometheus.
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotewritespec
      remoteWrite:
        - url: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9201/write"

      # Prometheus pod storage spec
      storageSpec:
        # Using PersistentVolumeClaim
        # disable mount sub path, use the root directory of pvc
        disableMountSubPath: true
        volumeClaimTemplate:
          spec:
            accessModes:
              - "ReadWriteOnce"
            resources:
              requests:
                storage: 8Gi

      # We've enabled annotation-based scraping by default for backward-compatibility
      # and to support the largest number of use-cases out-of-the-box.
      # We encourage people to use ServiceMonitors and PodMonitors for new components.
      # See discussion in: https://github.com/prometheus-operator/prometheus-operator/issues/1547
      # and more info: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#prometheusioscrape

      # If additional scrape configurations are already deployed in a single secret file you can use this section.
      # Expected values are the secret name and key
      # Cannot be used with additionalScrapeConfigs
      additionalScrapeConfigsSecret:
        enabled: true
        name: tobs-scrape-config
        key: additional-scrape-config.yaml

  # Values for configuring the deployment of Grafana
  # The Grafana Community chart is used and the guide for it
  # can be found at:
  #   https://github.com/grafana/helm-charts/blob/main/charts/grafana/README.md
  grafana:
    enabled: true
    # TODO(paulfantom): remove with kube-prometheus bump
    image:
      repository: grafana/grafana
      tag: 9.0.5
      pullPolicy: IfNotPresent
    resources: null
    envValueFrom:
      GRAFANA_PASSWORD:
        secretKeyRef:
          name: custom-secret-scripts
          key: GRAFANA_PASSWORD
    sidecar:
      datasources:
        enabled: true
        label: tobs_datasource
        labelValue: "true"
        # Disable Prometheus datasource by default as
        # Promscale is the default datasource
        defaultDatasourceEnabled: false
      dashboards:
        # option to enable multi-cluster support
        # in Grafana dashboards by default disabled
        multicluster:
          global:
            enabled: false
        enabled: true
        files:
          - dashboards/k8s-cluster.json
          - dashboards/k8s-hardware.json
          - dashboards/apm-dependencies.json
          - dashboards/apm-home.json
          - dashboards/apm-service-dependencies-downstream.json
          - dashboards/apm-service-dependencies-upstream.json
          - dashboards/apm-service-overview.json
          - dashboards/promscale.json
    adminUser: admin
    # To configure password externally refer to https://github.com/grafana/helm-charts/blob/6578497320d3c4672bab3a3c7fd38dffba1c9aba/charts/grafana/values.yaml#L340-L345
    adminPassword: ""
    persistence:
      type: pvc
      enabled: true
      accessModes:
        - ReadWriteOnce
    prometheus:
      datasource:
        enabled: true
        # By default url of data source is set to ts-prom connector instance
        # deployed with this chart. If a connector isn't used this should be
        # set to the prometheus-server.
        url: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9201"
    timescale:
      datasource:
        enabled: true
        user: grafana
        # leaving password empty will cause helm to generate a random password
        pass: ""
        dbName: *metricDB
        sslMode: require
        # By default the url/host is set to the db instance deployed
        # with this chart
        host: *dbHost
        port: 5432
    jaeger:
      # Endpoint for integrating jaeger datasource in grafana. This should point to HTTP endpoint, not gRPC.
      promscaleTracesQueryEndPoint: "{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9201"

  kube-state-metrics:
    image:
      repository: registry.k8s.io/kube-state-metrics/kube-state-metrics
      tag: v2.5.0
      pullPolicy: IfNotPresent
    # By default kube-state-metrics are scraped using
    # serviceMonitor disable annotation based scraping
    prometheusScrape: false
    resources: null

  prometheus-node-exporter:
    image:
      repository: quay.io/prometheus/node-exporter
      tag: v1.3.1
      pullPolicy: IfNotPresent
    # By default node-exporter are scraped using
    # serviceMonitor disable annotation based scraping
    service:
      annotations:
        prometheus.io/scrape: "false"
    resources: null

# Enable OpenTelemetry Operator
# If using tobs CLI you can enable otel with --enable-opentelemetry flag
opentelemetry-operator:
  enabled: true
  manager:
    image:
      repository: ghcr.io/open-telemetry/opentelemetry-operator/opentelemetry-operator
      tag: v0.56.0
    resources: null
    serviceMonitor:
      enabled: true
    prometheusRule:
      enabled: true
  instrumentation:
    pythonImage: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python:0.32b0
    javaImage: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-java:1.16.0
    nodejsImage: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-nodejs:0.27.0
  collector:
    # The default otel collector that will be deployed by helm once
    # the otel operator is in running state
    config: |
      receivers:
        jaeger:
          protocols:
            grpc:
            thrift_http:

        otlp:
          protocols:
            grpc:
            http:

      exporters:
        logging:
        otlp:
          endpoint: "{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9202"
          compression: none
          tls:
            insecure: true
        prometheusremotewrite:
          endpoint: "http://{{ .Release.Name }}-promscale-connector.{{ .Release.Namespace }}.svc:9201/write"
          tls:
            insecure: true

      processors:
        batch:

      service:
        pipelines:
          traces:
            receivers: [jaeger, otlp]
            exporters: [logging, otlp]
            processors: [batch]
          metrics:
            receivers: [otlp]
            processors: [batch]
            exporters: [prometheusremotewrite]
